# All shells have a number of built-in commands which are executed in the shell's own process. When you enter a command, the shell checks to see if it is a built-in command, and if so it executes it. If it is not a buit-in the shell forks a new process in which to execute the command.

# A signal handler is just a function that you compile together with the rest of the program. Instead of directly invoking the function, you use signal or sigaction to tell the operating system to call it when a signal arrives. This is known as establishing the handler

# Makefiles are a simple way to organize code compilation.

# Reap zombie processes using the following tutorial
http://www.microhowto.info/howto/reap_zombie_processes_using_a_sigchld_handler.html

# piped processes run concurrently 
Gilles @ http://unix.stackexchange.com/questions/37508/in-what-order-do-piped-commands-run
Piped commands run concurrently. When you run ps | grep …, it's the luck of the draw (or a matter of details of the workings of the shell combined with scheduler fine-tuning deep in the bowels of the kernel) as to whether ps or grep starts first, and in any case they continue to execute concurrently.

This is very commonly used to allow the second program to process data as it comes out from the first program, before the first program has completed its operation. For example

grep pattern very-large-file | tr a-z A-Z
begins to display the matching lines in uppercase even before grep has finished traversing the large file.

grep pattern very-large-file | head -n 1
displays the first matching line, and may stop processing well before grep has finished reading its input file.

If you read somewhere that piped programs run in sequence, flee this document. Piped programs run concurrently and always have.

# man 7 pipe | less
If a process attempts to read from an empty pipe, then read(2) will block until data is available.

# Atomic operations execute in a single step, without interruptions.
http://wiki.osdev.org/Atomic_operation
An atomic operation is an operation that will always be executed without any other process being able to read or change state that is read or changed during the operation. It is effectively executed as a single step, and is an important quality in a number of algorithms that deal with multiple indepent processes, both in synchronization and algorithms that update shared data without requiring synchronization.

# David Rodríguez - dribeas @ http://stackoverflow.com/questions/976015/why-do-i-need-to-close-fds-when-reading-and-writing-to-the-pipe
After performing the fork, all fds (file descriptors) are duplicated. Each process has both ends of the pipe open. If you only want to use one end you should close the other (if your process writes, close the read end).

# close files after calling dup2 @ http://stackoverflow.com/a/11518304
... Your second problem is that you need to close a plethora of file descriptors, one of which you no longer have a reference for.

So, you might need:

if ((pid = fork()) < 0)
    ...error...
else if (pid == 0)
{
    /* Be childish */
    if (in)
    {
        int fd0 = open(input, O_RDONLY, 0);
        dup2(fd0, STDIN_FILENO);
        close(fd0);
    }

    if (out)
    {
        int fd1 = creat(output, 0644);
        dup2(fd1, STDOUT_FILENO);
        close(fd1);
    }
    ...now the child has stdin coming from the input file, 
    ...stdout going to the output file, and no extra files open.
    ...it is safe to execute the command to be executed.
    execve(cmd[0], cmd, env);   // Or your preferred alternative
    fprintf(stderr, "Failed to exec %s\n", cmd[0]);
    exit(1);
}
else
{
    /* Be parental */
    ...wait for child to die, etc...
}
Before you do any of this, you should ensure that you've already flushed the shell's standard I/O channels, probably by using fflush(0), so that if the forked child writes to standard error because of a problem, there is no extraneous duplicated output.

# 
